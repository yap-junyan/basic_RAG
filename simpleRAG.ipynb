{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from custom_directory_loaders import DocxDirectoryLoader\n",
    "from sentence_transformers import CrossEncoder\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "from hashlib import sha256\n",
    "from typing import List\n",
    "from IPython.display import display, Markdown\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some functions to organise the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load documents\n",
    "def load_documents(file_directory: str, loaders: List) -> List[Document]:\n",
    "    documents = []\n",
    "    for loader in loaders:\n",
    "        documents.extend(loader(file_directory).load())\n",
    "        \n",
    "    return documents\n",
    "\n",
    "\n",
    "# Function to split documents into chunks and shift the raw files to the saved directory\n",
    "def prepare_documents(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 200) -> List[Document]:\n",
    "    # Breaking down documents into chunks\n",
    "    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = splitter.split_documents(documents)\n",
    "    \n",
    "    # Shifting the files in the 'raw' directory to the 'saved' directory\n",
    "    if not os.path.exists(\"./files_to_process/saved\"):\n",
    "        os.makedirs(\"./files_to_process/saved\")\n",
    "        \n",
    "    for file in os.listdir(\"./files_to_process/raw\"):\n",
    "        shutil.move(os.path.join(os.path.abspath(\"./files_to_process/raw\"), file), os.path.join(os.path.abspath(\"./files_to_process/saved\"), file))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "# Function to add documents to a collection in the vector database, and returns the vector database\n",
    "def add_to_vector_db(docs: List[Document], collection_name: str, \n",
    "                     embeddings: HuggingFaceInferenceAPIEmbeddings) -> Chroma:\n",
    "    # Initialise ChromaDB client\n",
    "    if not os.path.exists(\"./chroma_db_index\"):\n",
    "        os.mkdir(\"./chroma_db_index\")\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db_index\")\n",
    "    \n",
    "    # Get/Create a collection\n",
    "    chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "    )\n",
    "    \n",
    "    # Creating ids for docs\n",
    "    ids = [sha256(doc.page_content.encode('utf-8')).hexdigest() for doc in docs]\n",
    "\n",
    "    embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "        model=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
    "        api_key=os.getenv(\"HF_TOKEN\")\n",
    "        )\n",
    "\n",
    "    # Adding documents to the collection\n",
    "    db = Chroma.from_documents(\n",
    "        client=chroma_client,\n",
    "        collection_name=collection_name,\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        ids = ids\n",
    "    )\n",
    "    \n",
    "    return db\n",
    "\n",
    "\n",
    "# Function to initialise the existing vector database should there be no new documents to be added\n",
    "def get_vector_db(collection_name: str, embeddings: HuggingFaceInferenceAPIEmbeddings) -> Chroma:\n",
    "    if not os.path.exists(\"./chroma_db_index\"):\n",
    "        raise FileNotFoundError(\"No vector database found. Please add documents to the vector database.\")\n",
    "    \n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db_index\")\n",
    "    \n",
    "    db = Chroma(client=chroma_client, collection_name=collection_name, \n",
    "                embedding_function=embeddings,\n",
    "                )\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading our documents and storing them in a vector database (ChromaDB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our documents from the directory \n",
    "loaders = [DocxDirectoryLoader, PyPDFDirectoryLoader]\n",
    "\n",
    "if not os.path.exists(\"./files_to_process/raw\"):\n",
    "    os.makedirs(\"./files_to_process/raw\")\n",
    "documents = load_documents(file_directory=\"./files_to_process/raw/\", loaders=loaders)\n",
    "\n",
    "# Embeddings for indexing\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(model=\"sentence-transformers/all-MiniLM-l6-v2\", \n",
    "                                               api_key=os.getenv(\"HF_TOKEN\"))\n",
    "# CHROMA Collection Name\n",
    "collection_name = \"test_collection\"\n",
    "\n",
    "if documents:\n",
    "    docs = prepare_documents(documents)\n",
    "    print(f\"Number of documents: {len(docs)}\")\n",
    "    print(\"Example of a document: \\n\", docs[0].page_content)\n",
    "    db = add_to_vector_db(docs, collection_name=collection_name, embeddings=embeddings)\n",
    "\n",
    "# if there are no documents, initialise the existing vector database    \n",
    "else: \n",
    "    db = get_vector_db(collection_name=collection_name, embeddings=embeddings)\n",
    "    \n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retriever.invoke(\"Give me a brief summary of the document.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranking using a cross encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents to use as context in the prompt\n",
    "num_docs = 3\n",
    "\n",
    "def reranker(retrieved_documents: List[Document]):\n",
    "    \n",
    "    pairs = [[query, doc.page_content] for doc in retrieved_documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # # printing out to see change in order\n",
    "    # print(\"New Ordering:\")\n",
    "    # for o in np.argsort(scores)[::-1]:\n",
    "    #     print(o+1)\n",
    "    \n",
    "    # Selecting top n\n",
    "    top_n =  [retrieved_documents[i] for i, v in enumerate(np.argsort(scores)[::-1]) if v in range(num_docs)]\n",
    "    \n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing our prompt and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = dedent(\n",
    "       \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to aid in answering the question. \n",
    "       Keep the answer clear and concise, and support with examples if possible. Return the answer in a markdown format. \n",
    "       Question: {question}\n",
    "       Context: {context}\n",
    "       Answer:\"\"\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # (\"system\", template),\n",
    "    (\"human\", template)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running on LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", max_output_tokens=2048, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ollama with llama3 as the LLM\n",
    "model = Ollama(model=\"llama3\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating our chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format the retrieved documents in a format the LLM can take\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(reranker) | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "What is Message Passing Interface?\n",
    "\"\"\"\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onboarding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
